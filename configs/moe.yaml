# configs/moe.yaml

# --- Model Arguments (架构参数) ---
img_size: 128
patch_size: 4
embed_dim: 128            # 隐层维度 (Hidden Size)
text_dim: 768             # Roberta-base 的输出维度
num_experts: 4            # 专家总数
top_k: 2                  # 每次前向传播激活的专家数
fno_modes: 16             # FNO 专家保留的低频模态数
swin_window_size: 8       # Window Attention 的窗口大小
swin_num_heads: 4         # Attention 头数
mlp_ratio: 4.0            # MLP 膨胀系数
aux_loss_weight: 0.01     # 负载均衡 Loss 的权重
loss_type: "mse"          # 损失函数类型: "mse" 或 "l1"
max_train_samples: -1     # 训练样本上限 (-1 表示使用全部样本)

# --- Data Arguments (数据参数) ---
dataset_name: "fluids.incompressible.Gaussians" # 数据集名称 (需与 base.py 中的映射匹配)
data_path: "./data"       # 数据根目录
max_train_samples: null   # 设置具体的数字可用于快速调试 (如 1000)
max_eval_samples: null
ar_steps: 1               # 自回归步数 (1 表示单步预测)

# --- Training Arguments (Trainer 参数) ---
output_dir: "./checkpoints/moe_run_v1"
overwrite_output_dir: true
do_train: true
do_eval: true

# Optimization
learning_rate: 5.0e-4
weight_decay: 1.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.05
num_train_epochs: 50

# Batch Size & Performance
per_device_train_batch_size: 8  # 根据显存调整 (A100可设32, 3090可设8-16)
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
fp16: true                      # 开启混合精度训练 (节省显存)

# Logging & Saving
logging_steps: 50
save_steps: 500
eval_steps: 500
evaluation_strategy: "steps"
save_total_limit: 3             # 最多保留 3 个 Checkpoint
load_best_model_at_end: true

# System
seed: 42
dataloader_num_workers: 4
remove_unused_columns: false    # 【关键】设为 False，否则 Trainer 会把 text_embedding 等自定义列删掉
